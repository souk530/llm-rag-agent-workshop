"""
Option A: RAGã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰
PDFã‚„Webãƒšãƒ¼ã‚¸ã‹ã‚‰ã®æƒ…å ±æŠ½å‡ºã¨è³ªå•å¿œç­”ã‚·ã‚¹ãƒ†ãƒ 
"""

import os
import sys
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

import chromadb
import google.generativeai as genai
from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer
import PyPDF2
import requests
from bs4 import BeautifulSoup
import json
from typing import List, Dict
import time

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()

class RAGSystem:
    def __init__(self, collection_name="workshop_docs"):
        """RAGã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–"""
        
        # Google Gemini APIè¨­å®š
        genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))
        self.llm = genai.GenerativeModel('gemini-pro')
        
        # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«
        print("åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        
        # ChromaDBã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        self.chroma_client = chromadb.PersistentClient(path="./chroma_db")
        self.collection = self.chroma_client.get_or_create_collection(
            name=collection_name,
            metadata={"hnsw:space": "cosine"}
        )
        
        print("RAGã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–å®Œäº†")
    
    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
        try:
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                text = ""
                for page in pdf_reader.pages:
                    text += page.extract_text() + "\n"
                return text
        except Exception as e:
            print(f"PDFèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return ""
    
    def extract_text_from_url(self, url: str) -> str:
        """Webãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ä¸è¦ãªè¦ç´ ã‚’å‰Šé™¤
            for script in soup(["script", "style", "nav", "footer", "header"]):
                script.decompose()
            
            # ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
            text = soup.get_text()
            
            # ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
            lines = (line.strip() for line in text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            text = ' '.join(chunk for chunk in chunks if chunk)
            
            return text
            
        except Exception as e:
            print(f"Webãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return ""
    
    def chunk_text(self, text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²"""
        if not text.strip():
            return []
        
        chunks = []
        start = 0
        text_length = len(text)
        
        while start < text_length:
            end = start + chunk_size
            
            # æ–‡ã®å¢ƒç•Œã§åˆ‡ã‚‹
            if end < text_length:
                # æœ€å¾Œã®å¥ç‚¹ã¾ãŸã¯æ”¹è¡Œã‚’æ¢ã™
                last_period = text.rfind('ã€‚', start, end)
                last_newline = text.rfind('\n', start, end)
                
                if last_period > start:
                    end = last_period + 1
                elif last_newline > start:
                    end = last_newline
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            start = end - overlap
            if start <= 0:
                start = end
        
        return chunks
    
    def add_documents(self, documents: List[Dict[str, str]]):
        """æ–‡æ›¸ã‚’ãƒ™ã‚¯ãƒˆãƒ«DBã«è¿½åŠ """
        print(f"{len(documents)}å€‹ã®æ–‡æ›¸ã‚’å‡¦ç†ä¸­...")
        
        all_chunks = []
        all_metadatas = []
        all_ids = []
        
        for i, doc in enumerate(documents):
            content = doc['content']
            source = doc.get('source', f'document_{i}')
            doc_type = doc.get('type', 'unknown')
            
            # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
            chunks = self.chunk_text(content)
            print(f"{source}: {len(chunks)}ãƒãƒ£ãƒ³ã‚¯")
            
            for j, chunk in enumerate(chunks):
                chunk_id = f"{source}_chunk_{j}"
                all_chunks.append(chunk)
                all_metadatas.append({
                    "source": source,
                    "type": doc_type,
                    "chunk_index": j,
                    "char_count": len(chunk)
                })
                all_ids.append(chunk_id)
        
        if not all_chunks:
            print("è¿½åŠ ã™ã‚‹æ–‡æ›¸ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        # åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
        print("åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆä¸­...")
        embeddings = self.embedding_model.encode(all_chunks).tolist()
        
        # ChromaDBã«è¿½åŠ 
        self.collection.add(
            documents=all_chunks,
            metadatas=all_metadatas,
            ids=all_ids,
            embeddings=embeddings
        )
        
        print(f"âœ… {len(all_chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ãƒ™ã‚¯ãƒˆãƒ«DBã«è¿½åŠ å®Œäº†")
    
    def search_relevant_chunks(self, query: str, top_k: int = 5) -> List[Dict]:
        """é–¢é€£ã™ã‚‹ãƒãƒ£ãƒ³ã‚¯ã‚’æ¤œç´¢"""
        # ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
        query_embedding = self.embedding_model.encode([query]).tolist()
        
        # æ¤œç´¢å®Ÿè¡Œ
        results = self.collection.query(
            query_embeddings=query_embedding,
            n_results=top_k
        )
        
        # çµæœã‚’æ•´ç†
        relevant_chunks = []
        for i in range(len(results['documents'][0])):
            relevant_chunks.append({
                'content': results['documents'][0][i],
                'metadata': results['metadatas'][0][i],
                'distance': results['distances'][0][i]
            })
        
        return relevant_chunks
    
    def generate_answer(self, query: str, context_chunks: List[Dict]) -> str:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’åŸºã«å›ç­”ã‚’ç”Ÿæˆ"""
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰
        context = "\n\n".join([
            f"ã€å‡ºå…¸: {chunk['metadata']['source']}ã€‘\n{chunk['content']}"
            for chunk in context_chunks
        ])
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
        prompt = f"""
ä»¥ä¸‹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’åŸºã«ã€è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚

ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ:
{context}

è³ªå•: {query}

å›ç­”ã®éš›ã¯ä»¥ä¸‹ã®ç‚¹ã«æ³¨æ„ã—ã¦ãã ã•ã„ï¼š
1. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å«ã¾ã‚Œã‚‹æƒ…å ±ã®ã¿ã‚’ä½¿ç”¨
2. æƒ…å ±æºã‚’æ˜è¨˜
3. ã‚ã‹ã‚‰ãªã„å ´åˆã¯ã€Œæä¾›ã•ã‚ŒãŸæƒ…å ±ã§ã¯å›ç­”ã§ãã¾ã›ã‚“ã€ã¨å›ç­”
4. ç°¡æ½”ã§åˆ†ã‹ã‚Šã‚„ã™ã„å›ç­”ã‚’å¿ƒãŒã‘ã‚‹

å›ç­”:
"""
        
        try:
            response = self.llm.generate_content(prompt)
            return response.text
        except Exception as e:
            return f"å›ç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}"
    
    def query(self, question: str, top_k: int = 5, show_sources: bool = True) -> Dict:
        """è³ªå•å¿œç­”ã®å®Ÿè¡Œ"""
        print(f"\nè³ªå•: {question}")
        print("é–¢é€£æƒ…å ±ã‚’æ¤œç´¢ä¸­...")
        
        # é–¢é€£ãƒãƒ£ãƒ³ã‚¯æ¤œç´¢
        relevant_chunks = self.search_relevant_chunks(question, top_k)
        
        if not relevant_chunks:
            return {
                "answer": "é–¢é€£ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚",
                "sources": []
            }
        
        # å›ç­”ç”Ÿæˆ
        print("å›ç­”ã‚’ç”Ÿæˆä¸­...")
        answer = self.generate_answer(question, relevant_chunks)
        
        # ã‚½ãƒ¼ã‚¹æƒ…å ±
        sources = []
        if show_sources:
            print("\nå‚ç…§ã—ãŸæƒ…å ±æº:")
            for i, chunk in enumerate(relevant_chunks, 1):
                source_info = f"[{i}] {chunk['metadata']['source']} (é¡ä¼¼åº¦: {1-chunk['distance']:.3f})"
                print(source_info)
                sources.append({
                    "source": chunk['metadata']['source'],
                    "content": chunk['content'][:200] + "...",
                    "similarity": 1 - chunk['distance']
                })
        
        return {
            "answer": answer,
            "sources": sources,
            "relevant_chunks": relevant_chunks
        }
    
    def get_collection_stats(self) -> Dict:
        """ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®çµ±è¨ˆæƒ…å ±"""
        count = self.collection.count()
        
        if count > 0:
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰çµ±è¨ˆã‚’å–å¾—
            all_data = self.collection.get()
            sources = set()
            doc_types = {}
            
            for metadata in all_data['metadatas']:
                sources.add(metadata['source'])
                doc_type = metadata.get('type', 'unknown')
                doc_types[doc_type] = doc_types.get(doc_type, 0) + 1
            
            return {
                "total_chunks": count,
                "unique_sources": len(sources),
                "document_types": doc_types,
                "sources": list(sources)
            }
        
        return {"total_chunks": 0, "unique_sources": 0, "document_types": {}, "sources": []}

def demo_with_sample_data():
    """ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ã®ãƒ‡ãƒ¢"""
    
    # RAGã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    rag = RAGSystem("demo_collection")
    
    # ã‚µãƒ³ãƒ—ãƒ«æ–‡æ›¸
    sample_documents = [
        {
            "content": """
äººå·¥çŸ¥èƒ½ï¼ˆAIï¼‰ã¯ã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ãŒäººé–“ã®ã‚ˆã†ãªçŸ¥çš„è¡Œå‹•ã‚’ç¤ºã™æŠ€è¡“åˆ†é‡ã§ã™ã€‚
æ©Ÿæ¢°å­¦ç¿’ã€è‡ªç„¶è¨€èªå‡¦ç†ã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ãªã©ã®æŠ€è¡“ã‚’çµ„ã¿åˆã‚ã›ã¦ã€è¤‡é›‘ãªå•é¡Œã‚’è§£æ±ºã—ã¾ã™ã€‚
è¿‘å¹´ã€æ·±å±¤å­¦ç¿’ã®ç™ºå±•ã«ã‚ˆã‚Šã€AIã®æ€§èƒ½ã¯é£›èºçš„ã«å‘ä¸Šã—ã¦ã„ã¾ã™ã€‚

AIã®ä¸»è¦ãªå¿œç”¨åˆ†é‡ï¼š
1. ç”»åƒèªè­˜ï¼šåŒ»ç™‚è¨ºæ–­ã€è‡ªå‹•é‹è»¢è»Šã®è¦–è¦šã‚·ã‚¹ãƒ†ãƒ 
2. è‡ªç„¶è¨€èªå‡¦ç†ï¼šæ©Ÿæ¢°ç¿»è¨³ã€ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã€æ–‡æ›¸è¦ç´„
3. æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ï¼šECã‚µã‚¤ãƒˆã€å‹•ç”»é…ä¿¡ã‚µãƒ¼ãƒ“ã‚¹
4. éŸ³å£°èªè­˜ï¼šã‚¹ãƒãƒ¼ãƒˆã‚¹ãƒ”ãƒ¼ã‚«ãƒ¼ã€éŸ³å£°ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ

AIã®èª²é¡Œï¼š
- èª¬æ˜å¯èƒ½æ€§ï¼šAIã®åˆ¤æ–­æ ¹æ‹ ãŒä¸æ˜ç¢º
- ãƒã‚¤ã‚¢ã‚¹ï¼šå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®åã‚ŠãŒçµæœã«å½±éŸ¿
- ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ï¼šå€‹äººæƒ…å ±ã®å–ã‚Šæ‰±ã„
- é›‡ç”¨ã¸ã®å½±éŸ¿ï¼šè‡ªå‹•åŒ–ã«ã‚ˆã‚‹è·ã®å¤‰åŒ–
            """,
            "source": "AIåŸºç¤è³‡æ–™",
            "type": "æ•™æ"
        },
        {
            "content": """
æ©Ÿæ¢°å­¦ç¿’ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã—ã¦ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã™ã‚‹AIã®æ‰‹æ³•ã§ã™ã€‚

ä¸»è¦ãªæ©Ÿæ¢°å­¦ç¿’ã®ç¨®é¡ï¼š

1. æ•™å¸«ã‚ã‚Šå­¦ç¿’ï¼ˆSupervised Learningï¼‰
   - æ­£è§£ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦å­¦ç¿’
   - åˆ†é¡ã€å›å¸°å•é¡Œ
   - ä¾‹ï¼šã‚¹ãƒ‘ãƒ ãƒ¡ãƒ¼ãƒ«åˆ¤å®šã€æ ªä¾¡äºˆæ¸¬

2. æ•™å¸«ãªã—å­¦ç¿’ï¼ˆUnsupervised Learningï¼‰
   - æ­£è§£ãªã—ã§ãƒ‡ãƒ¼ã‚¿ã®æ§‹é€ ã‚’ç™ºè¦‹
   - ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã€æ¬¡å…ƒå‰Šæ¸›
   - ä¾‹ï¼šé¡§å®¢ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã€ç•°å¸¸æ¤œçŸ¥

3. å¼·åŒ–å­¦ç¿’ï¼ˆReinforcement Learningï¼‰
   - è©¦è¡ŒéŒ¯èª¤ã‚’é€šã˜ã¦æœ€é©ãªè¡Œå‹•ã‚’å­¦ç¿’
   - å ±é…¬ã‚’æœ€å¤§åŒ–ã™ã‚‹æ–¹ç­–ã‚’ç¿’å¾—
   - ä¾‹ï¼šã‚²ãƒ¼ãƒ AIã€ãƒ­ãƒœãƒƒãƒˆåˆ¶å¾¡

æ·±å±¤å­¦ç¿’ã¯æ©Ÿæ¢°å­¦ç¿’ã®ä¸€åˆ†é‡ã§ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’å¤šå±¤åŒ–ã—ãŸæ‰‹æ³•ã§ã™ã€‚
ç”»åƒèªè­˜ã€è‡ªç„¶è¨€èªå‡¦ç†ã§ç‰¹ã«å„ªã‚ŒãŸæ€§èƒ½ã‚’ç™ºæ®ã—ã¾ã™ã€‚
            """,
            "source": "æ©Ÿæ¢°å­¦ç¿’å…¥é–€",
            "type": "æ•™æ"
        },
        {
            "content": """
è‡ªç„¶è¨€èªå‡¦ç†ï¼ˆNLPï¼‰ã¯ã€äººé–“ã®è¨€èªã‚’ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ãŒç†è§£ãƒ»å‡¦ç†ã™ã‚‹æŠ€è¡“ã§ã™ã€‚

NLPã®ä¸»è¦ã‚¿ã‚¹ã‚¯ï¼š

1. å½¢æ…‹ç´ è§£æï¼šæ–‡ã‚’å˜èªã«åˆ†å‰²
2. æ§‹æ–‡è§£æï¼šæ–‡ã®æ–‡æ³•æ§‹é€ ã‚’è§£æ
3. æ„å‘³è§£æï¼šæ–‡ã®æ„å‘³ã‚’ç†è§£
4. æ©Ÿæ¢°ç¿»è¨³ï¼šè¨€èªé–“ã®ç¿»è¨³
5. æ„Ÿæƒ…åˆ†æï¼šãƒ†ã‚­ã‚¹ãƒˆã®æ„Ÿæƒ…ã‚’åˆ¤å®š
6. è³ªå•å¿œç­”ï¼šè³ªå•ã«å¯¾ã™ã‚‹å›ç­”ç”Ÿæˆ
7. æ–‡æ›¸è¦ç´„ï¼šé•·ã„æ–‡æ›¸ã®è¦ç´„ä½œæˆ

æœ€è¿‘ã®ç™ºå±•ï¼š
- Transformerï¼šAttentionæ©Ÿæ§‹ã«ã‚ˆã‚‹é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«
- BERTï¼šåŒæ–¹å‘ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼
- GPTï¼šç”Ÿæˆå‹äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«
- ChatGPTï¼šå¯¾è©±å‹AI

å¿œç”¨ä¾‹ï¼š
- æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³
- éŸ³å£°ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ
- è‡ªå‹•ç¿»è¨³
- ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ
- æ–‡æ›¸åˆ†é¡
            """,
            "source": "NLPæŠ€è¡“æ¦‚è¦",
            "type": "æŠ€è¡“æ–‡æ›¸"
        }
    ]
    
    # æ–‡æ›¸ã‚’RAGã‚·ã‚¹ãƒ†ãƒ ã«è¿½åŠ 
    rag.add_documents(sample_documents)
    
    # çµ±è¨ˆæƒ…å ±è¡¨ç¤º
    stats = rag.get_collection_stats()
    print(f"\nğŸ“Š ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³çµ±è¨ˆ:")
    print(f"ç·ãƒãƒ£ãƒ³ã‚¯æ•°: {stats['total_chunks']}")
    print(f"æ–‡æ›¸æ•°: {stats['unique_sources']}")
    print(f"æ–‡æ›¸ã‚¿ã‚¤ãƒ—: {stats['document_types']}")
    
    # ã‚µãƒ³ãƒ—ãƒ«è³ªå•ã§ãƒ†ã‚¹ãƒˆ
    sample_questions = [
        "AIã®ä¸»è¦ãªå¿œç”¨åˆ†é‡ã¯ä½•ã§ã™ã‹ï¼Ÿ",
        "æ•™å¸«ã‚ã‚Šå­¦ç¿’ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ",
        "è‡ªç„¶è¨€èªå‡¦ç†ã®æœ€è¿‘ã®ç™ºå±•ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„",
        "æ©Ÿæ¢°å­¦ç¿’ã®ç¨®é¡ã‚’æ•™ãˆã¦ãã ã•ã„",
        "Transformerã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ"
    ]
    
    print("\nğŸ¤– è³ªå•å¿œç­”ãƒ‡ãƒ¢")
    print("=" * 50)
    
    for question in sample_questions:
        result = rag.query(question)
        print(f"\nğŸ’¬ {question}")
        print(f"ğŸ¤– {result['answer']}")
        print("-" * 50)

def interactive_demo():
    """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªãƒ‡ãƒ¢"""
    
    print("=== RAGã‚·ã‚¹ãƒ†ãƒ  ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ‡ãƒ¢ ===")
    rag = RAGSystem("interactive_collection")
    
    while True:
        print("\né¸æŠã—ã¦ãã ã•ã„:")
        print("1. æ–‡æ›¸ã‚’è¿½åŠ  (ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›)")
        print("2. PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½åŠ ")
        print("3. Webãƒšãƒ¼ã‚¸ã‚’è¿½åŠ ")
        print("4. è³ªå•ã™ã‚‹")
        print("5. çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º")
        print("6. çµ‚äº†")
        
        choice = input("\né¸æŠ (1-6): ").strip()
        
        if choice == "1":
            print("\næ–‡æ›¸ã®å†…å®¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ (ç©ºè¡Œã§çµ‚äº†):")
            content_lines = []
            while True:
                line = input()
                if line.strip() == "":
                    break
                content_lines.append(line)
            
            if content_lines:
                content = "\n".join(content_lines)
                source = input("æ–‡æ›¸åã‚’å…¥åŠ›: ").strip() or "æ‰‹å…¥åŠ›æ–‡æ›¸"
                
                documents = [{
                    "content": content,
                    "source": source,
                    "type": "æ‰‹å…¥åŠ›"
                }]
                rag.add_documents(documents)
        
        elif choice == "2":
            pdf_path = input("PDFãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’å…¥åŠ›: ").strip()
            if os.path.exists(pdf_path):
                content = rag.extract_text_from_pdf(pdf_path)
                if content:
                    documents = [{
                        "content": content,
                        "source": os.path.basename(pdf_path),
                        "type": "PDF"
                    }]
                    rag.add_documents(documents)
                else:
                    print("PDFã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
            else:
                print("ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        elif choice == "3":
            url = input("Webãƒšãƒ¼ã‚¸ã®URLã‚’å…¥åŠ›: ").strip()
            if url.startswith("http"):
                content = rag.extract_text_from_url(url)
                if content:
                    documents = [{
                        "content": content,
                        "source": url,
                        "type": "Webãƒšãƒ¼ã‚¸"
                    }]
                    rag.add_documents(documents)
                else:
                    print("Webãƒšãƒ¼ã‚¸ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
            else:
                print("æœ‰åŠ¹ãªURLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
        
        elif choice == "4":
            stats = rag.get_collection_stats()
            if stats['total_chunks'] == 0:
                print("ã¾ãšæ–‡æ›¸ã‚’è¿½åŠ ã—ã¦ãã ã•ã„")
                continue
                
            question = input("\nè³ªå•ã‚’å…¥åŠ›: ").strip()
            if question:
                result = rag.query(question)
                print(f"\nå›ç­”: {result['answer']}")
        
        elif choice == "5":
            stats = rag.get_collection_stats()
            print(f"\nğŸ“Š çµ±è¨ˆæƒ…å ±:")
            print(f"ç·ãƒãƒ£ãƒ³ã‚¯æ•°: {stats['total_chunks']}")
            print(f"æ–‡æ›¸æ•°: {stats['unique_sources']}")
            print(f"æ–‡æ›¸ã‚¿ã‚¤ãƒ—: {stats['document_types']}")
            if stats['sources']:
                print("æ–‡æ›¸ä¸€è¦§:")
                for source in stats['sources']:
                    print(f"  - {source}")
        
        elif choice == "6":
            print("ãƒ‡ãƒ¢ã‚’çµ‚äº†ã—ã¾ã™")
            break
        
        else:
            print("ç„¡åŠ¹ãªé¸æŠã§ã™")

def batch_processing_demo():
    """ãƒãƒƒãƒå‡¦ç†ãƒ‡ãƒ¢"""
    
    print("=== ãƒãƒƒãƒå‡¦ç†ãƒ‡ãƒ¢ ===")
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    data_dir = Path("sample_data")
    data_dir.mkdir(exist_ok=True)
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
    sample_files = {
        "ai_history.txt": """
äººå·¥çŸ¥èƒ½ã®æ­´å²

1950å¹´ä»£ï¼š
- ã‚¢ãƒ©ãƒ³ãƒ»ãƒãƒ¥ãƒ¼ãƒªãƒ³ã‚°ãŒãƒãƒ¥ãƒ¼ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆã‚’ææ¡ˆ
- ã€Œæ©Ÿæ¢°ã¯è€ƒãˆã‚‹ã“ã¨ãŒã§ãã‚‹ã‹ï¼Ÿã€ã¨ã„ã†å•ã„ã‚’æèµ·

1960å¹´ä»£ï¼š
- åˆæœŸã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ é–‹ç™º
- ELIZAï¼šåˆæœŸã®è‡ªç„¶è¨€èªå‡¦ç†ãƒ—ãƒ­ã‚°ãƒ©ãƒ 

1980å¹´ä»£ï¼š
- ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ ã®å•†æ¥­åŒ–
- ç¬¬äºŒæ¬¡AIãƒ–ãƒ¼ãƒ 

1990å¹´ä»£ï¼š
- æ©Ÿæ¢°å­¦ç¿’æ‰‹æ³•ã®ç™ºå±•
- ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å†æ³¨ç›®

2000å¹´ä»£ï¼š
- æ·±å±¤å­¦ç¿’ã®åŸºç¤ç ”ç©¶
- ãƒ“ãƒƒã‚°ãƒ‡ãƒ¼ã‚¿ã®æ™®åŠ

2010å¹´ä»£ï¼š
- æ·±å±¤å­¦ç¿’ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¹ãƒ«ãƒ¼
- ImageNetã€AlphaGoã®æˆåŠŸ

2020å¹´ä»£ï¼š
- å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã®ç™»å ´
- GPTã€BERTã€ChatGPTã®æ™®åŠ
        """,
        
        "ml_algorithms.txt": """
æ©Ÿæ¢°å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ä¸€è¦§

ç·šå½¢å›å¸°ï¼ˆLinear Regressionï¼‰ï¼š
- é€£ç¶šå€¤äºˆæ¸¬ã®åŸºæœ¬æ‰‹æ³•
- è§£é‡ˆã—ã‚„ã™ã„
- ç·šå½¢é–¢ä¿‚ã‚’ä»®å®š

æ±ºå®šæœ¨ï¼ˆDecision Treeï¼‰ï¼š
- åˆ†é¡ãƒ»å›å¸°ä¸¡æ–¹ã«å¯¾å¿œ
- ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã§è§£é‡ˆã—ã‚„ã™ã„
- éå­¦ç¿’ã—ã‚„ã™ã„

ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆï¼ˆRandom Forestï¼‰ï¼š
- æ±ºå®šæœ¨ã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•
- é«˜ã„ç²¾åº¦
- ç‰¹å¾´é‡é‡è¦åº¦ãŒã‚ã‹ã‚‹

ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ãƒˆãƒ«ãƒã‚·ãƒ³ï¼ˆSVMï¼‰ï¼š
- é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã«å¼·ã„
- ã‚«ãƒ¼ãƒãƒ«é–¢æ•°ã§éç·šå½¢åˆ†é›¢
- è¨ˆç®—ã‚³ã‚¹ãƒˆãŒé«˜ã„

kè¿‘å‚æ³•ï¼ˆk-NNï¼‰ï¼š
- ã‚·ãƒ³ãƒ—ãƒ«ãªåˆ†é¡ãƒ»å›å¸°æ‰‹æ³•
- å±€æ‰€çš„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ‰ãˆã‚‹
- è¨ˆç®—é‡ãŒå¤šã„

ãƒŠã‚¤ãƒ¼ãƒ–ãƒ™ã‚¤ã‚ºï¼ˆNaive Bayesï¼‰ï¼š
- ç¢ºç‡çš„åˆ†é¡æ‰‹æ³•
- ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã§æœ‰åŠ¹
- ç‰¹å¾´é‡ç‹¬ç«‹æ€§ã‚’ä»®å®š

ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼š
- è„³ã®ç¥çµŒç´°èƒã‚’æ¨¡å€£
- éç·šå½¢é–¢ä¿‚ã‚’å­¦ç¿’
- æ·±å±¤å­¦ç¿’ã®åŸºç¤
        """
    }
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
    for filename, content in sample_files.items():
        with open(data_dir / filename, 'w', encoding='utf-8') as f:
            f.write(content)
    
    print(f"ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ {data_dir} ã«ä½œæˆã—ã¾ã—ãŸ")
    
    # RAGã‚·ã‚¹ãƒ†ãƒ ã§å‡¦ç†
    rag = RAGSystem("batch_collection")
    
    documents = []
    for file_path in data_dir.glob("*.txt"):
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        documents.append({
            "content": content,
            "source": file_path.name,
            "type": "ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«"
        })
    
    rag.add_documents(documents)
    
    # ãƒ†ã‚¹ãƒˆè³ªå•
    test_questions = [
        "äººå·¥çŸ¥èƒ½ã®æ­´å²ã«ãŠã‘ã‚‹é‡è¦ãªå‡ºæ¥äº‹ã‚’æ•™ãˆã¦ãã ã•ã„",
        "1950å¹´ä»£ã®AIç ”ç©¶ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„",
        "ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã®ç‰¹å¾´ã¯ä½•ã§ã™ã‹ï¼Ÿ",
        "ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã«é©ã—ãŸã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"
    ]
    
    print("\nğŸ“ ãƒãƒƒãƒå‡¦ç†çµæœ:")
    for question in test_questions:
        result = rag.query(question, show_sources=False)
        print(f"\nQ: {question}")
        print(f"A: {result['answer']}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    
    if not os.getenv('GOOGLE_API_KEY'):
        print("ERROR: GOOGLE_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print(".envãƒ•ã‚¡ã‚¤ãƒ«ã§APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„")
        return
    
    print("ğŸš€ RAGã‚·ã‚¹ãƒ†ãƒ  ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ§ãƒƒãƒ—")
    print("=" * 40)
    
    while True:
        print("\nãƒ‡ãƒ¢ã‚’é¸æŠã—ã¦ãã ã•ã„:")
        print("1. ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ã®ãƒ‡ãƒ¢")
        print("2. ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ‡ãƒ¢")
        print("3. ãƒãƒƒãƒå‡¦ç†ãƒ‡ãƒ¢")
        print("4. çµ‚äº†")
        
        choice = input("\né¸æŠ (1-4): ").strip()
        
        if choice == "1":
            demo_with_sample_data()
        elif choice == "2":
            interactive_demo()
        elif choice == "3":
            batch_processing_demo()
        elif choice == "4":
            print("ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ§ãƒƒãƒ—ã‚’çµ‚äº†ã—ã¾ã™")
            break
        else:
            print("ç„¡åŠ¹ãªé¸æŠã§ã™")

if __name__ == "__main__":
    main()
