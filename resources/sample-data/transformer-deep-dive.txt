深層学習とTransformer技術解説

第1章 深層学習の基礎

1.1 ニューラルネットワークの歴史
深層学習は、1940年代に提案されたパーセプトロンから発展した技術である。1980年代の誤差逆伝播法の発明により実用化が進み、2006年のジェフリー・ヒントンによる深層信念ネットワークの提案から現代の深層学習ブームが始まった。

1.2 活性化関数の進歩
- シグモイド関数：σ(x) = 1/(1+e^(-x))
- tanh関数：tanh(x) = (e^x - e^(-x))/(e^x + e^(-x))
- ReLU関数：ReLU(x) = max(0, x)
- Leaky ReLU：f(x) = max(αx, x) where α < 1
- Swish：f(x) = x * σ(βx)

ReLUの登場により勾配消失問題が大幅に改善され、深いネットワークの学習が可能になった。

1.3 正規化手法
バッチ正規化（Batch Normalization）：
BN(x) = γ * (x - μ)/σ + β

レイヤー正規化（Layer Normalization）：
各サンプルの特徴量次元で正規化

グループ正規化（Group Normalization）：
チャンネルをグループに分けて正規化

1.4 最適化アルゴリズム
SGD（確率的勾配降下法）：
θ(t+1) = θ(t) - η * ∇L(θ(t))

Adam（Adaptive Moment Estimation）：
m(t) = β₁ * m(t-1) + (1-β₁) * g(t)
v(t) = β₂ * v(t-1) + (1-β₂) * g(t)²
θ(t+1) = θ(t) - η * m(t)/√(v(t) + ε)

第2章 畳み込みニューラルネットワーク（CNN）

2.1 畳み込み演算
畳み込み：(f * g)(t) = ∫ f(τ)g(t-τ)dτ

離散畳み込み：(f * g)[n] = Σ f[m]g[n-m]

2次元畳み込み：
(I * K)[i,j] = ΣΣ I[m,n]K[i-m,j-n]

2.2 プーリング
最大プーリング：各窓の最大値を取る
平均プーリング：各窓の平均値を取る
ストライドプーリング：ステップ幅を指定

2.3 主要なCNNアーキテクチャ

LeNet-5（1998）：
- 手書き数字認識用
- 畳み込み層 + プーリング層 + 全結合層

AlexNet（2012）：
- ImageNet優勝
- ReLU、Dropout、Data Augmentation使用
- GPU活用による大規模学習

VGGNet（2014）：
- 3x3フィルターを重ねた設計
- 深さの重要性を実証
- VGG-16、VGG-19が有名

ResNet（2015）：
- 残差接続（Skip Connection）
- 超深層ネットワーク（152層）を実現
- 恒等写像の学習により勾配消失を解決

DenseNet（2017）：
- 全ての層が全ての後続層に接続
- 特徴量の再利用による効率化

2.4 転移学習
事前学習済みモデルを新しいタスクに適用：
1. Feature Extraction：下位層を固定
2. Fine-tuning：全体を微調整
3. 新しいデータセットのサイズに応じて選択

第3章 回帰型ニューラルネットワーク（RNN）

3.1 基本RNN
隠れ状態の更新：
h(t) = tanh(W_hh * h(t-1) + W_ih * x(t) + b_h)
y(t) = W_hy * h(t) + b_y

3.2 RNNの問題点
- 勾配消失・爆発問題
- 長期依存関係の学習困難
- 並列処理が困難

3.3 LSTM（Long Short-Term Memory）
ゲート機構による記憶制御：

忘却ゲート：f(t) = σ(W_f * [h(t-1), x(t)] + b_f)
入力ゲート：i(t) = σ(W_i * [h(t-1), x(t)] + b_i)
候補値：C̃(t) = tanh(W_C * [h(t-1), x(t)] + b_C)
セル状態：C(t) = f(t) * C(t-1) + i(t) * C̃(t)
出力ゲート：o(t) = σ(W_o * [h(t-1), x(t)] + b_o)
隠れ状態：h(t) = o(t) * tanh(C(t))

3.4 GRU（Gated Recurrent Unit）
LSTMの簡略版：

リセットゲート：r(t) = σ(W_r * [h(t-1), x(t)])
更新ゲート：z(t) = σ(W_z * [h(t-1), x(t)])
候補状態：h̃(t) = tanh(W * [r(t) * h(t-1), x(t)])
隠れ状態：h(t) = (1 - z(t)) * h(t-1) + z(t) * h̃(t)

3.5 双方向RNN
前向きと後向きの情報を統合：
h_forward(t) = RNN_forward(x(1), ..., x(t))
h_backward(t) = RNN_backward(x(T), ..., x(t))
h(t) = [h_forward(t); h_backward(t)]

第4章 Attention機構

4.1 Attention機構の動機
RNNの固定長ベクトルの限界を解決：
- 長いシーケンスでの情報損失
- 入力の重要な部分への注意集中

4.2 基本的なAttention
重み計算：e(t,s) = score(h(t), h̄(s))
正規化：α(t,s) = exp(e(t,s)) / Σ exp(e(t,k))
コンテキスト：c(t) = Σ α(t,s) * h̄(s)

スコア関数の種類：
- Dot Product：score(h_t, h̄_s) = h_t^T h̄_s
- General：score(h_t, h̄_s) = h_t^T W_a h̄_s
- Additive：score(h_t, h̄_s) = v_a^T tanh(W_a[h_t; h̄_s])

4.3 Self-Attention
同一シーケンス内での関係性を学習：
Q = XW_Q（Query）
K = XW_K（Key）
V = XW_V（Value）

Attention(Q,K,V) = softmax(QK^T/√d_k)V

4.4 Multi-Head Attention
複数の注意機構を並列実行：
MultiHead(Q,K,V) = Concat(head_1, ..., head_h)W_O
where head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)

第5章 Transformer Architecture

5.1 Transformerの構成要素
Transformer = Encoder + Decoder

エンコーダー：
- Multi-Head Self-Attention
- Position-wise Feed-Forward Network
- Residual Connection + Layer Normalization

デコーダー：
- Masked Multi-Head Self-Attention
- Multi-Head Cross-Attention
- Position-wise Feed-Forward Network
- Residual Connection + Layer Normalization

5.2 位置エンコーディング
Sinusoidal Position Encoding：
PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))

学習可能な位置エンコーディング：
PE = Embedding(position)

5.3 Masked Attention
デコーダーでの未来情報の隠蔽：
mask = -∞ (i > j)
scores = scores + mask

5.4 Position-wise FFN
各位置で独立に適用：
FFN(x) = max(0, xW_1 + b_1)W_2 + b_2

第6章 事前学習済み言語モデル

6.1 BERT（Bidirectional Encoder Representations from Transformers）
双方向の文脈理解：

Masked Language Model (MLM)：
- 入力の15%をマスク
- マスクされた単語を予測

Next Sentence Prediction (NSP)：
- 文ペアの連続性を予測
- [CLS] token for classification

6.2 GPT（Generative Pre-trained Transformer）
自己回帰的生成モデル：
- 左から右への言語モデリング
- Causal Masking使用
- Zero-shot、Few-shot学習

GPT-2の改良点：
- Layer Normalizationの位置変更
- 語彙サイズとコンテキスト長の拡大

GPT-3の特徴：
- 1750億パラメータ
- In-context Learning
- 多様なタスクでの高性能

6.3 T5（Text-to-Text Transfer Transformer）
全てのタスクをテキスト生成として統一：
- 入力：タスク説明 + 入力テキスト
- 出力：回答テキスト
- 大規模コーパスでの事前学習

6.4 RoBERTa
BERTの改良版：
- 動的マスキング
- NSPタスクの除去
- より大きなバッチサイズ
- より長い学習

第7章 現代のTransformerアーキテクチャ

7.1 効率的なAttention機構

Sparse Attention：
- 全てのトークンペアではなく、パターンベースの接続
- BigBird、Longformerなど

Linear Attention：
- O(n²)からO(n)への計算量削減
- Linformer、Performer

7.2 Vision Transformer (ViT)
画像をパッチ序列として処理：
1. 画像を固定サイズパッチに分割
2. 各パッチを線形埋め込み
3. 位置エンコーディング追加
4. Transformerエンコーダーで処理

7.3 DETR（Detection Transformer）
物体検出のEnd-to-End学習：
- CNNバックボーン + Transformer
- オブジェクトクエリによる検出
- バイpartiteマッチングによる学習

7.4 Switch Transformer
Sparse Expert Model：
- MoE（Mixture of Experts）
- 条件付き計算で効率化
- 大規模パラメータ化

第8章 大規模言語モデル（LLM）

8.1 スケーリング法則
モデル性能とリソースの関係：
- パラメータ数
- データサイズ
- 計算量

Chinchilla論文の知見：
最適な学習にはデータとパラメータのバランスが重要

8.2 Emergent Abilities
一定規模を超えると現れる能力：
- Few-shot学習
- Chain-of-Thought推論
- 指示に従う能力

8.3 インストラクションチューニング
人間の指示に従うよう微調整：
- InstructGPT
- ChatGPT
- Claude

8.4 RLHF（Reinforcement Learning from Human Feedback）
人間のフィードバックで強化学習：
1. 教師ありファインチューニング
2. 報酬モデル学習
3. PPOによる強化学習

第9章 マルチモーダルTransformer

9.1 Vision-Language Model
画像とテキストの統合処理：
- CLIP：対照学習によるマルチモーダル表現
- DALL-E：テキストから画像生成
- Flamingo：Few-shot学習でのマルチモーダル理解

9.2 音声処理Transformer
- Wav2Vec 2.0：自己教師あり音声表現学習
- Whisper：多言語音声認識
- SpeechT5：音声のUnified Model

第10章 Transformerの応用

10.1 機械翻訳
- 品質の大幅向上
- 低資源言語対応
- 多言語統一モデル

10.2 要約生成
- 抽出型要約から生成型要約へ
- 長文書の要約
- 多文書要約

10.3 質問応答
- 読解型QA
- オープンドメインQA
- 対話型QA

10.4 コード生成
- GitHub Copilot
- CodeT5
- AlphaCode

第11章 最適化と効率化

11.1 モデル圧縮
知識蒸留：
大きなモデル（教師）から小さなモデル（生徒）への知識転移

プルーニング：
重要でない重みを除去してモデルサイズ削減

量子化：
重みの精度を下げて計算量とメモリ使用量を削減

11.2 高速推論
- 動的推論：早期終了機構
- 投機的デコーディング：並列生成
- KV-Cache：キー・バリューの再利用

11.3 分散学習
- データ並列：バッチを複数GPUで分割
- モデル並列：モデルを複数GPUで分割
- パイプライン並列：レイヤーごとに並列化

第12章 今後の発展

12.1 アーキテクチャの進歩
- より効率的な注意機構
- 新しい正規化手法
- 異なるトークン化手法

12.2 学習手法の改善
- より効果的な事前学習
- マルチタスク学習の発展
- メタ学習の統合

12.3 応用分野の拡大
- 科学計算への応用
- 創薬・材料設計
- ロボティクス

結論
Transformerアーキテクチャは自然言語処理を革新し、現在では画像、音声、さらには科学計算まで幅広い分野で活用されている。その成功の鍵は、Attention機構による効率的な長距離依存関係の学習と、大規模な並列計算の実現にある。今後もさらなる効率化と新たな応用分野での発展が期待される。
