機械学習アルゴリズム詳細解説

第1章 線形回帰（Linear Regression）

1.1 概要
線形回帰は、入力変数と出力変数の間の線形関係をモデル化する手法である。最もシンプルで理解しやすい機械学習アルゴリズムの一つであり、多くの実用的な問題に適用できる。

1.2 数学的定式化
単回帰の場合：y = ax + b
重回帰の場合：y = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ + ε

ここで、βᵢは回帰係数、εは誤差項である。

1.3 最小二乗法
回帰係数は最小二乗法により求める：
- 残差の二乗和を最小化
- 解析的に解が求まる
- 正規方程式：β = (X^T X)^(-1) X^T y

1.4 応用例
- 売上予測：広告費と売上の関係
- 不動産価格予測：立地、面積、築年数から価格を予測
- 医療：BMIと血圧の関係分析

1.5 利点と制限
利点：
- 解釈しやすい
- 計算が高速
- 過学習しにくい

制限：
- 線形関係のみ表現可能
- 外れ値に敏感
- 多重共線性の問題

第2章 決定木（Decision Tree）

2.1 概要
決定木は、データを段階的に分割することで予測を行う手法である。樹木状の構造で表現され、直感的に理解しやすい。

2.2 分割基準
- 情報利得（Information Gain）
- ジニ不純度（Gini Impurity）
- 分散削減

情報利得の計算：
IG(T,a) = H(T) - Σ |Tᵥ|/|T| H(Tᵥ)

2.3 アルゴリズム
1. 最適な分割属性を選択
2. データを分割
3. 各サブセットに対して再帰的に実行
4. 停止条件まで継続

停止条件：
- 最大深度に達した
- ノードのサンプル数が最小値以下
- 情報利得が閾値以下

2.4 剪定（Pruning）
過学習を防ぐため、不要な枝を削除：
- 事前剪定：成長中に停止
- 事後剪定：完全な木から削除

2.5 応用例
- 医療診断：症状から病気を診断
- 信用審査：顧客情報から信用度を判定
- マーケティング：顧客特性から購買行動を予測

第3章 ランダムフォレスト（Random Forest）

3.1 概要
ランダムフォレストは、複数の決定木を組み合わせたアンサンブル手法である。各木をランダムに学習させることで、汎化性能を向上させる。

3.2 アルゴリズム
1. ブートストラップサンプリングでデータセット作成
2. 各ノードで特徴量をランダム選択
3. 複数の決定木を学習
4. 予測時は多数決（分類）または平均（回帰）

3.3 重要な特徴量選択
各特徴量の重要度を計算：
- ジニ重要度
- 順列重要度
- SHAP値

3.4 利点
- 高い予測精度
- 過学習しにくい
- 特徴量重要度が得られる
- 欠損値に対して頑健

3.5 応用例
- 遺伝子発現データ解析
- 金融リスク評価
- 画像分類の特徴量として

第4章 サポートベクトルマシン（SVM）

4.1 概要
SVMは、最適な分離超平面を見つけることで分類を行う手法である。マージンを最大化することで汎化性能を高める。

4.2 線形SVM
目的関数：
min (1/2)||w||² + C Σξᵢ
subject to: yᵢ(wᵀxᵢ + b) ≥ 1 - ξᵢ

4.3 非線形SVM（カーネル法）
カーネル関数により高次元空間に写像：
- 線形カーネル：K(x,x') = xᵀx'
- RBFカーネル：K(x,x') = exp(-γ||x-x'||²)
- 多項式カーネル：K(x,x') = (γxᵀx' + r)^d

4.4 パラメータ調整
- C：正則化パラメータ
- γ：RBFカーネルの幅
- degree：多項式カーネルの次数

4.5 応用例
- テキスト分類
- 生体認証
- 化学物質の分類

第5章 k近傍法（k-NN）

5.1 概要
k近傍法は、新しいデータポイントに最も近いk個の訓練データを用いて予測を行う非パラメトリック手法である。

5.2 アルゴリズム
1. 新しいデータポイントから各訓練データまでの距離を計算
2. 距離の小さい順にk個選択
3. 分類：多数決、回帰：平均値

5.3 距離尺度
- ユークリッド距離：√Σ(xᵢ-yᵢ)²
- マンハッタン距離：Σ|xᵢ-yᵢ|
- ミンコフスキー距離：(Σ|xᵢ-yᵢ|^p)^(1/p)

5.4 kの選択
- 小さすぎる：ノイズに敏感
- 大きすぎる：境界が曖昧
- 交差検証で最適値を選択

5.5 高速化手法
- KD-Tree
- Ball Tree
- LSH（Locality Sensitive Hashing）

第6章 ナイーブベイズ（Naive Bayes）

6.1 概要
ナイーブベイズは、ベイズの定理に基づく確率的分類手法である。特徴量間の独立性を仮定することで計算を簡略化する。

6.2 ベイズの定理
P(y|x) = P(x|y)P(y) / P(x)

ナイーブベイズの仮定：
P(x₁,x₂,...,xₙ|y) = ∏P(xᵢ|y)

6.3 種類
- ガウシアンナイーブベイズ：連続値データ
- 多項ナイーブベイズ：カウントデータ
- ベルヌーイナイーブベイズ：バイナリデータ

6.4 応用例
- スパムフィルタ
- テキスト分類
- 感情分析
- 医療診断

第7章 クラスタリング

7.1 k-means
目的関数：
J = ΣΣ||xᵢ - μⱼ||²

アルゴリズム：
1. クラスタ中心を初期化
2. 各点を最近のクラスタに割り当て
3. クラスタ中心を更新
4. 収束まで繰り返し

7.2 階層クラスタリング
- 凝集型：下から上へ
- 分割型：上から下へ

距離の定義：
- 単連結：最短距離
- 完全連結：最長距離
- 平均連結：平均距離

7.3 DBSCAN
密度ベースクラスタリング：
- コア点：近傍に十分な点がある
- 境界点：コア点の近傍にある
- ノイズ点：どちらでもない

第8章 次元削減

8.1 主成分分析（PCA）
分散を最大化する方向を見つける：
1. データを標準化
2. 共分散行列を計算
3. 固有値・固有ベクトルを求める
4. 寄与率の高い成分を選択

8.2 t-SNE
確率的近傍埋め込み：
- 高次元での近傍関係を低次元で保持
- 可視化に優れる
- 非線形変換

8.3 UMAP
Uniform Manifold Approximation and Projection：
- t-SNEより高速
- 大域構造を保持
- パラメータ調整しやすい

第9章 異常検知

9.1 統計的手法
- z-score：標準化した値
- 四分位範囲：Q1-1.5*IQR, Q3+1.5*IQR
- マハラノビス距離：多変量での外れ値

9.2 機械学習手法
- One-Class SVM：正常データのみで学習
- Isolation Forest：孤立させやすさで異常度計算
- Local Outlier Factor：局所的な密度異常

9.3 深層学習手法
- オートエンコーダー：再構成誤差
- VAE：変分オートエンコーダー
- GAN：生成的対抗ネットワーク

第10章 評価指標

10.1 分類問題
- 精度（Accuracy）：(TP+TN)/(TP+TN+FP+FN)
- 適合率（Precision）：TP/(TP+FP)
- 再現率（Recall）：TP/(TP+FN)
- F1スコア：2*Precision*Recall/(Precision+Recall)
- AUC-ROC：ROC曲線の下の面積

10.2 回帰問題
- 平均絶対誤差（MAE）：Σ|yᵢ-ŷᵢ|/n
- 平均二乗誤差（MSE）：Σ(yᵢ-ŷᵢ)²/n
- 決定係数（R²）：1 - SS_res/SS_tot

10.3 クラスタリング
- シルエット係数：クラスタ内結合度とクラスタ間分離度
- 調整ランド指数：真のクラスタとの一致度
- 相互情報量：クラスタ間の情報量

第11章 実装のベストプラクティス

11.1 データ前処理
- 欠損値処理：削除、補完、フラグ化
- 外れ値処理：除去、変換、上限設定
- 正規化：標準化、Min-Max scaling、Robust scaling
- エンコーディング：One-hot、Label、Target encoding

11.2 特徴量エンジニアリング
- 特徴量選択：フィルタ法、ラッパー法、埋め込み法
- 特徴量生成：多項式、交互作用、ドメイン知識
- 次元削減：PCA、t-SNE、UMAP

11.3 モデル選択と評価
- 交差検証：k-fold、Stratified、Time series split
- ハイパーパラメータ調整：Grid search、Random search、Bayesian optimization
- アンサンブル：Bagging、Boosting、Stacking

11.4 実運用での注意点
- データ漂流：時間とともにデータ分布が変化
- モデル劣化：性能の監視と再学習
- 説明可能性：LIME、SHAP、Permutation importance
- 公平性：バイアス検出と軽減
