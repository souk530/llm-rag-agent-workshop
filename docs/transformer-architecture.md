# Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®åŸºç¤

## ğŸ§  æ¦‚è¦

Transformerã¯ç¾ä»£ã®LLMã®åŸºç›¤ã¨ãªã‚‹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã™ã€‚2017å¹´ã«ç™ºè¡¨ã•ã‚ŒãŸ"Attention Is All You Need"è«–æ–‡ã§åˆã‚ã¦ç´¹ä»‹ã•ã‚Œã¾ã—ãŸã€‚

## ğŸ” ä¸»è¦ãªæ§‹æˆè¦ç´ 

### 1. Attentionæ©Ÿæ§‹

Attentionã¯ã€å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ç•°ãªã‚‹éƒ¨åˆ†ã«ã€Œæ³¨æ„ã€ã‚’å‘ã‘ã‚‹ä»•çµ„ã¿ã§ã™ã€‚

#### Self-Attention
- æ–‡ä¸­ã®å„å˜èªãŒä»–ã®ã™ã¹ã¦ã®å˜èªã¨ã®é–¢ä¿‚æ€§ã‚’å­¦ç¿’
- æ–‡è„ˆç†è§£ãŒå¤§å¹…ã«å‘ä¸Š

#### Multi-Head Attention
- è¤‡æ•°ã®ç•°ãªã‚‹ã€Œæ³¨æ„ã€ã®è¦³ç‚¹ã‚’ä¸¦è¡Œã—ã¦å­¦ç¿’
- ã‚ˆã‚Šè±Šã‹ãªè¡¨ç¾ãŒå¯èƒ½

### 2. ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã¨åŸ‹ã‚è¾¼ã¿è¡¨ç¾

```python
# ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã®ä¾‹
text = "ä»Šæ—¥ã¯è‰¯ã„å¤©æ°—ã§ã™"
tokens = ["ä»Šæ—¥", "ã¯", "è‰¯ã„", "å¤©æ°—", "ã§ã™"]
token_ids = [1234, 56, 789, 101, 234]
```

### 3. ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°

- ãƒˆãƒ¼ã‚¯ãƒ³ã®é †åºæƒ…å ±ã‚’ä¿æŒ
- Sinusoidalé–¢æ•°ã‚„Learnable Embeddingã‚’ä½¿ç”¨

## ğŸ”„ Transformerã®å‹•ä½œãƒ•ãƒ­ãƒ¼

1. **å…¥åŠ›å‡¦ç†**: ãƒ†ã‚­ã‚¹ãƒˆ â†’ ãƒˆãƒ¼ã‚¯ãƒ³ â†’ åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«
2. **ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼**: Self-Attentionã§æ–‡è„ˆã‚’ç†è§£
3. **ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼**: æ¬¡ã®å˜èªã‚’äºˆæ¸¬
4. **å‡ºåŠ›**: ç¢ºç‡åˆ†å¸ƒã‹ã‚‰æœ€é©ãªå˜èªã‚’é¸æŠ

## ğŸ’¡ ãªãœTransformerãŒé©æ–°çš„ã ã£ãŸã®ã‹

- **ä¸¦åˆ—å‡¦ç†**: RNNã¨é•ã„ã€å…¨ã¦ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’åŒæ™‚ã«å‡¦ç†å¯èƒ½
- **é•·è·é›¢ä¾å­˜**: é›¢ã‚ŒãŸå˜èªåŒå£«ã®é–¢ä¿‚ã‚‚åŠ¹ç‡çš„ã«å­¦ç¿’
- **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£**: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’å¢—ã‚„ã—ã‚„ã™ã„è¨­è¨ˆ

## ğŸ› ï¸ å®Ÿè£…ä¾‹

```python
import torch
import torch.nn as nn

class SimpleAttention(nn.Module):
    def __init__(self, d_model):
        super().__init__()
        self.d_model = d_model
        self.w_q = nn.Linear(d_model, d_model)
        self.w_k = nn.Linear(d_model, d_model)
        self.w_v = nn.Linear(d_model, d_model)
    
    def forward(self, x):
        Q = self.w_q(x)  # Query
        K = self.w_k(x)  # Key
        V = self.w_v(x)  # Value
        
        # Attentionè¨ˆç®—
        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_model ** 0.5)
        attention_weights = torch.softmax(scores, dim=-1)
        output = torch.matmul(attention_weights, V)
        
        return output
```

## ğŸ“š å‚è€ƒè³‡æ–™

- [Attention Is All You Need (åŸè«–æ–‡)](https://arxiv.org/abs/1706.03762)
- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)
- [Transformerã®æ•°å­¦çš„è§£èª¬](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
